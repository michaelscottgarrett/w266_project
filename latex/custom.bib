% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{hubbard2020measuring,
  title={Measuring and improving cyber defense using the MITRE ATT \& CK framework},
  author={Hubbard, John},
  journal={SANS Whitepaper},
  year={2020}
}

@online{tram,
    title = {Threat report ATT&CK mapper (tram)},
    author = {MITRE Engenuity}, 
    url = {https://mitre-engenuity.org/cybersecurity/center-for-threat-informed-defense/our-work/threat-report-attck-mapper-tram/#project-resources},
    urldate = {2023, August 28}
}

@article{beltagy2019scibert,
  title={SciBERT: A pretrained language model for scientific text},
  author={Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  journal={arXiv preprint arXiv:1903.10676},
  year={2019}
}

@article{grigorescu2022cve2att,
  title={Cve2att\&ck: Bert-based mapping of cves to mitre att\&ck techniques},
  author={Grigorescu, Octavian and Nica, Andreea and Dascalu, Mihai and Rughinis, Razvan},
  journal={Algorithms},
  volume={15},
  number={9},
  pages={314},
  year={2022},
  publisher={MDPI}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@online{secBERT,
    title = {Jackaduma/Secbert: Pretrained Bert Model for cyber security text, learned cybersecurity knowledge.},
    author = {Jackaduma. (n.d.)},
    url = {https://github.com/jackaduma/SecBERT/tree/main} 
}

@online{data,
    title = {Hoangcuongnguyen/CTI-to-mitre-dataset},
    author = {Nguyen, H. C. (n.d.)},
    url = {https://huggingface.co/datasets/HoangCuongNguyen/CTI-to-MITRE-dataset}
}